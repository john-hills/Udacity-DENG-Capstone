{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Engineering Capstone Project: Version 1.0\n",
    "\n",
    "### Tools and Libraries: PostgreSQL, PySpark, Psycopg2, Pandas, SQLAlchemy.\n",
    "\n",
    "This file is my first run at the project. I've chosen to do this on a local installation of PostgreSQL. Primarily, this is an exercise to get PostgreSQL up and running for later use. It'll be a great option when I don't want to incur the costs associated with cloud services on AWS / Azure / Watson. Go up a level in this repository to see the solutions I developed using different technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Process\n",
    "\n",
    "Prior to running this on your machine, you need to install PostgreSQL and Spark. Spark requires Java so if you don't already have that, you'll need to install it too. These are the steps that worked for me. My adventures in StackOverflow have told me that no two Windows 10 PCs are the same so I expect that your process will look slightly different in the end.\n",
    "\n",
    "1. Install PostgreSQL locally. You can get it here:  \n",
    "    https://www.enterprisedb.com/downloads/postgres-postgresql-downloads\n",
    "2. Superuser = 'postgres'. Password = 'postgres'. \n",
    "3. Create local postgres database.\n",
    "4. Add details to a configuration file, dl.cfg.\n",
    "5. If running on Windows, you may need to install or update Java. Instructions:               \n",
    "    https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1\n",
    "6. I had to get the JDK. Make sure you get JDK version 8, not later as Spark is not compatible with later versions yet. Instructions in the first link, download files in the second:  \n",
    "    https://docs.oracle.com/javase/7/docs/webnotes/install/windows/jdk-installation-windows.html  \n",
    "    https://www.oracle.com/java/technologies/javase-jdk14-downloads.html\n",
    "7. An important step in the JDK install is to add JAVA_HOME as an environment variable. You can't have a space in the path name for this. I got around it by referring to '\\Program Files\\' as '\\Progra~1\\'. \n",
    "    + While you're here, you can set the SPARK_HOME environment variable too.\n",
    "8. It was also necessary to download 7Zip (or similar) to unpack the downloaded Apache Spark gdz file:  \n",
    "    https://www.7-zip.org/download.html\n",
    "9. More useful notes here:  \n",
    "    https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3\n",
    "10. Get the PostgreSQL JDBC driver. This enables writing from a Spark dataframe to a PostgreSQL database:  \n",
    "    https://jdbc.postgresql.org/download.html\n",
    "11. Notes on the JDBC driver:  \n",
    "    https://jdbc.postgresql.org/documentation/81/setup.html\n",
    "\n",
    "**NB. In the function that creates a SparkSession below, I've included two '.config' lines. The second of these is necessary to handle the JDBC driver.**\n",
    "\n",
    "12. Import the SAS data immigration files. I copied them from the Udacity workspace to my local drive by running this line in a Terminal:\n",
    "\n",
    "> !zip -r data.zip ../../data/18-83510-I94-Data-2016  \n",
    "\n",
    "    This created a zip file containing all files within the sas_data folder on the Udacity workspace. I downloaded and unzipped this file to work with the data. The sas_data folder was created locally in the same place as this notebook file. I also downloaded I94_SAS_Labels_Descriptions.SAS to the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run these installs if you haven't already installed the packages on your machine.\n",
    "\n",
    "#!pip install pyspark\n",
    "#pip install psycopg2\n",
    "#conda install pyspark\n",
    "#!pip install findspark # run once all of the spark installation steps have been followed.\n",
    "\n",
    "import psycopg2 # PostgreSQL database adapter for Python\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT # <-- ADD THIS LINE\n",
    "import configparser # to work with the configuration file\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, DoubleType, IntegerType, TimestampType,FloatType)\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import pyspark\n",
    "from sqlalchemy import create_engine # allows writing to PostgreSQL from Spark\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg') # edit this file to include your own values.\n",
    "\n",
    "#the variables below are set from those you entered into 'dl.cfg'\n",
    "POSTGRES_USER=config['POSTGRES']['POSTGRES_USER']\n",
    "POSTGRES_PASSWORD=config['POSTGRES']['POSTGRES_PASSWORD']\n",
    "POSTGRES_DATABASE=config['POSTGRES']['POSTGRES_DATABASE']\n",
    "SAS_DATA_LOCATION=config['OTHER']['SAS_DATA_LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional step to clear tables from PostgreSQL before you start. \n",
    "exec_psql('drop table if exists temperatures;')\n",
    "exec_psql('drop table if exists demographics;')\n",
    "exec_psql('drop table if exists visa_group_sas;')\n",
    "exec_psql('drop table if exists fact_immigration;')\n",
    "exec_psql('drop table if exists fact_temperatures;')\n",
    "exec_psql('drop table if exists airports_iata;')\n",
    "exec_psql('drop table if exists dim_date;')\n",
    "exec_psql('drop table if exists immigration;')\n",
    "exec_psql('drop table if exists country_sas;')\n",
    "exec_psql('drop table if exists airport_city_state;')\n",
    "exec_psql('drop table if exists dim_arrival_mode;')\n",
    "exec_psql('drop table if exists airport_sas;')\n",
    "exec_psql('drop table if exists dim_country;')\n",
    "exec_psql('drop table if exists dim_airport;')\n",
    "exec_psql('drop table if exists dim_visa_group;')\n",
    "exec_psql('drop table if exists arrival_mode_sas;')\n",
    "exec_psql('drop table if exists dim_us_state;')\n",
    "exec_psql('drop table if exists us_state_sas;')\n",
    "exec_psql('drop table if exists fact_immigration;')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to run SQL queries on a local installation of PostgreSQL.\n",
    "\n",
    "This function handles select queries differently to DDL/DML statements, returning a recordset as a pandas dataframe for the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_psql(input_sql = \"select '*No SQL statement provided.';\",print_cols = \"No\"): \n",
    "    \"\"\"\n",
    "    Connect to postgres database, execute query, option: show table definition, \n",
    "    show requested rows (if 'select'), close connection.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        con = psycopg2.connect(host='localhost',database=POSTGRES_DATABASE, user=POSTGRES_USER,password=POSTGRES_PASSWORD)\n",
    "        con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        \n",
    "        #handle action queries       \n",
    "        if input_sql.strip()[0:6].lower() != 'select':\n",
    "            cur = con.cursor()\n",
    "            cur.execute(input_sql)\n",
    "            cur.close\n",
    "            con.close\n",
    "            print(f'Executed non-select query: {input_sql}')\n",
    "        else:\n",
    "            df = pd.read_sql(input_sql, con)\n",
    "            # this argument allows you to view the schema of a table\n",
    "            if print_cols != \"No\":\n",
    "                df_c = pd.read_sql(f\"select column_name, data_type, character_maximum_length \\\n",
    "                from INFORMATION_SCHEMA.COLUMNS where table_name ='{print_cols}';\", con)\n",
    "                print(df_c)\n",
    "            pd.options.display.max_columns = None # allow the user to see all of the data requested on screen\n",
    "            pd.options.display.max_rows = None # allow the user to see all of the data requested on screen\n",
    "            df = pd.read_sql(input_sql, con)\n",
    "            con.close\n",
    "            return df\n",
    "            pd.reset_option('^display.', silent=True) # reset the options to their defaults. here, this affects max_columns and max_rows\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data from source to PostgreSQL staging tables\n",
    "\n",
    "The next two cells define two functions. The first creates a Spark session. The second utilises a Spark session to build a Spark dataframe from an input file and then writes the data to a specified table in a specified PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create a spark session in which to work on the data.\"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"C:/Progra~1/Java/jdk1.8.0_251/postgresql-42.2.14.jar\")\\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spark_to_postgresql(read_format,fpath,tname,delimiter=','):\n",
    "    \"\"\"\n",
    "    Create a Spark dataframe from an input file. \n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    read_format: E.g. csv.\n",
    "    fpath: Full path for your input file, e.g. 'c:\\your_file.csv'.\n",
    "    tname: The name of the table to write to in PostgreSQL.\n",
    "    delimiter: E.g. ','\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    df =spark.read.format(read_format) \\\n",
    "                  .option(\"header\",\"true\") \\\n",
    "                  .option(\"delimiter\",delimiter) \\\n",
    "                  .load(fpath)\n",
    "    exec_psql(f\"drop table if exists {tname}\")\n",
    "    df.write.jdbc(f\"jdbc:postgresql:{POSTGRES_DATABASE}\", tname,\n",
    "    properties={\"user\": POSTGRES_USER, \"password\": POSTGRES_PASSWORD})\n",
    "    print(tname,f': copied to postgres {POSTGRES_DATABASE}')\n",
    "    print(' ')\n",
    "    #display ten rows from the postgres table for the user to view\n",
    "    exec_psql(f\"select * from {tname} limit 10\",tname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the above function to create PySpark dataframes of the SAS immigration data and copy this to the local PostgreSQL database.    \n",
    "    \n",
    "    Spark is generally used for handling large datasets. Moving a large dataset to a local PostgreSQL database would be slow and certainly not best practice. However, the dataset here is not large while I am demonstrating the use of Spark to read and write a dataset.\n",
    "\n",
    "Similarly, dataframes are also created of three more provided source files, each of which was provided on the Udacity workspace as a CSV and which I copied locally to work with:\n",
    "   + Temperature Data\n",
    "   + Demographics\n",
    "   + Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed non-select query: drop table if exists immigration\n",
      "immigration : copied to postgres udacity_capstone\n",
      " \n",
      "   column_name         data_type character_maximum_length\n",
      "0        cicid  double precision                     None\n",
      "1        i94yr  double precision                     None\n",
      "2       i94mon  double precision                     None\n",
      "3       i94cit  double precision                     None\n",
      "4       i94res  double precision                     None\n",
      "5      i94port              text                     None\n",
      "6      arrdate  double precision                     None\n",
      "7      i94mode  double precision                     None\n",
      "8      i94addr              text                     None\n",
      "9      depdate  double precision                     None\n",
      "10      i94bir  double precision                     None\n",
      "11     i94visa  double precision                     None\n",
      "12       count  double precision                     None\n",
      "13    dtadfile              text                     None\n",
      "14    visapost              text                     None\n",
      "15       occup              text                     None\n",
      "16     entdepa              text                     None\n",
      "17     entdepd              text                     None\n",
      "18     entdepu              text                     None\n",
      "19     matflag              text                     None\n",
      "20     biryear  double precision                     None\n",
      "21     dtaddto              text                     None\n",
      "22      gender              text                     None\n",
      "23      insnum              text                     None\n",
      "24     airline              text                     None\n",
      "25      admnum  double precision                     None\n",
      "26       fltno              text                     None\n",
      "27    visatype              text                     None\n",
      "Executed non-select query: drop table if exists temperatures\n",
      "temperatures : copied to postgres udacity_capstone\n",
      " \n",
      "                     column_name data_type character_maximum_length\n",
      "0                            _c0      text                     None\n",
      "1                             dt      text                     None\n",
      "2             AverageTemperature      text                     None\n",
      "3  AverageTemperatureUncertainty      text                     None\n",
      "4                           City      text                     None\n",
      "5                        Country      text                     None\n",
      "6                       Latitude      text                     None\n",
      "7                      Longitude      text                     None\n",
      "Executed non-select query: drop table if exists demographics\n",
      "demographics : copied to postgres udacity_capstone\n",
      " \n",
      "               column_name data_type character_maximum_length\n",
      "0                     City      text                     None\n",
      "1                    State      text                     None\n",
      "2               Median Age      text                     None\n",
      "3          Male Population      text                     None\n",
      "4        Female Population      text                     None\n",
      "5         Total Population      text                     None\n",
      "6       Number of Veterans      text                     None\n",
      "7             Foreign-born      text                     None\n",
      "8   Average Household Size      text                     None\n",
      "9               State Code      text                     None\n",
      "10                    Race      text                     None\n",
      "11                   Count      text                     None\n",
      "Executed non-select query: drop table if exists airports_iata\n",
      "airports_iata : copied to postgres udacity_capstone\n",
      " \n",
      "     column_name data_type character_maximum_length\n",
      "0          ident      text                     None\n",
      "1           type      text                     None\n",
      "2           name      text                     None\n",
      "3   elevation_ft      text                     None\n",
      "4      continent      text                     None\n",
      "5    iso_country      text                     None\n",
      "6     iso_region      text                     None\n",
      "7   municipality      text                     None\n",
      "8       gps_code      text                     None\n",
      "9      iata_code      text                     None\n",
      "10    local_code      text                     None\n",
      "11   coordinates      text                     None\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary of arguments for the four input files\n",
    "parameters_dict = {'immigration': {'read_format':'com.github.saurfang.sas.spark','fpath':SAS_DATA_LOCATION + 'i94_apr16_sub.sas7bdat','delimiter':','},\n",
    "       'temperatures': {'read_format':'csv','fpath':'GlobalLandTemperaturesByCity.csv','delimiter':','},\n",
    "       'demographics': {'read_format':'csv','fpath':'us-cities-demographics.csv','delimiter':';'},\n",
    "       'airports_iata': {'read_format':'csv','fpath':'airport-codes_csv.csv','delimiter':','}\n",
    "      }\n",
    "\n",
    "# iterate through the dictionary, writing each dataframe to PostgreSQL. we can look at the schema for each staging table too.\n",
    "for k in parameters_dict.keys():\n",
    "    spark_to_postgresql(parameters_dict[k]['read_format'],parameters_dict[k]['fpath'],k,parameters_dict[k]['delimiter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframes from the SAS look-up data in the file 'I94_SAS_Labels_Descriptions.SAS' (copied locally from Udacity workspace). These are then copied to PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the lookup data\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "\n",
    "def code_mapper(file, idx):\n",
    "    \"\"\"\n",
    "    From udacity knowledge base.\n",
    "    Sources lookup values from the SAS file. \n",
    "    Returns a dictionary.\n",
    "    \"\"\"\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "\n",
    "i94cit = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business',\n",
    "'2': 'Pleasure',\n",
    "'3' : 'Student'}\n",
    "#correct value that's not right\n",
    "i94cit['582'] = 'MEXICO'\n",
    "\n",
    "#this creates a list that contains the dictionaries for each field and the new description for it.\n",
    "pairs = [(i94cit, 'country'), (i94port,'airport') #use i94cit/i94res : same lookup values\n",
    "         ,(i94mode,'arrival_mode'),(i94addr,'us_state'),(i94visa,'visa_group')]\n",
    "\n",
    "#get the lookup data into a pd dataframe and copy to PostgreSQL\n",
    "for (data, col) in pairs:\n",
    "    df = pd.DataFrame.from_dict(data, orient='index', columns=[col])\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    cols = df.columns\n",
    "    cols.values[0] = cols[1] + '_cd'\n",
    "    df.columns = cols\n",
    "    exec_psql(f'drop table if exists {col}_sas;')\n",
    "    engine = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(POSTGRES_USER,POSTGRES_PASSWORD,POSTGRES_DATABASE))\n",
    "    df.to_sql(col + '_sas', engine)\n",
    "    print(col,f': copied to postgres {POSTGRES_DATABASE}')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tables created so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_psql(\"\"\"\n",
    "    SELECT --table_schema,\n",
    "   -- ',',\n",
    "    table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE 1=1\n",
    "    and table_type = 'BASE TABLE'\n",
    "    AND table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "  \n",
    "    ;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Schema\n",
    "\n",
    "![](database_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "![](data_dictionary.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the schema\n",
    "\n",
    "The aim of the schema design I've created is to maximise the potential for analysis using the provided data sources. The focus of the analysis is expected to be the fact_immigration table. Analysts are expected to be looking for insights into instances of persons travelling to the USA. Questions that may be asked include:\n",
    "\n",
    "    + Who are the people who are travelling?\n",
    "    + Where have the people travelled from?\n",
    "    + Do hotter cities see more visitors?  \n",
    "      - Is this only relative to the city size?\n",
    "    + Do the demographics of a destination city/state make it more or less popular for visitors?\n",
    "    + Is there a correlation between temperature at departure city and temperature at destination city?\n",
    "\n",
    "There are significant limitations and problems with the data provided. I'd recommend using left joins from the fact tables at all times. Specifically, caution should be taken in these circumstances:\n",
    "   + Joining from fact_demographics to dim_airport. This will work in some cases and the idea is to enhance the immigration data rather than to use the demographic data as a basis for analysis.\n",
    "   + Joining from fact_temperature to dim_airport. Again, this will work in some cases and the idea is to enhance the immigration data rather than to use the demographic data as a basis for analysis. Additional caution is necessary here as you are only joining with 'city' when there are multiple cases of the same city name appearing in more than one state. \n",
    "\n",
    "Necessary enhancements that were beyond the scope of this project: \n",
    "   - Add state to fact_temperature. This could be done using the latitude and longitude data in the source data. Necessary to distinguish between cities with the same name in different states.\n",
    "   - Thorough cleaning of the airport codes data. This is mostly used 'as is' but it is likely that some of the codes are incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the schema\n",
    "\n",
    "Now that we have all of the data in PostgreSQL, let's make some changes to the formats and apply some filtering to get the schema that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fact_immigration from staging table. apply format changes to conserve disk space. do some cleaning\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists fact_immigration;\n",
    "    create table fact_immigration\n",
    "    as\n",
    "    select cast(cicid as int) as unique_id\n",
    "    ,cast('01-jan-1960' as date) + cast(arrdate as int) as arrival_dt\n",
    "    ,cast('01-jan-1960' as date) + cast(depdate as int) as departure_dt\n",
    "    ,fltno as flight_no\n",
    "    ,cast(i94visa as smallint) as visa_group_cd\n",
    "    ,cast(i94port as varchar(3)) as airport_cd\n",
    "    --handle some invalid data\n",
    "    ,to_date(case when dtaddto = 'D/S' then null\n",
    "            when dtaddto = '183' then null\n",
    "            when right(dtaddto,1) = 'D' then null\n",
    "            else dtaddto end,'MMDDYYYY') as admitted_dt\n",
    "    ,airline \n",
    "    ,cast(i94addr as varchar(2)) as state_cd\n",
    "    ,cast(coalesce(i94mode,9) as smallint) as arrival_mode_cd --replace NaN with 'Not reported'\n",
    "    ,entdepa as arrival_flag\n",
    "    ,entdepd as departure_flag\n",
    "    ,i94bir as age_on_arrival\n",
    "    ,biryear as birth_year\n",
    "    ,cast(i94cit as int) as country_birth_cd\n",
    "    ,cast(i94res as int) as country_residence_cd\n",
    "    ,gender as gender\n",
    "    ,occup as occupation\n",
    "    ,admnum as adm_num\n",
    "\n",
    "    from immigration\n",
    "    ;\n",
    "    ALTER TABLE fact_immigration ADD PRIMARY KEY (unique_id);\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dim_date table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_date\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_date;\n",
    "    create table dim_date\n",
    "    as\n",
    "    select distinct \n",
    "    arrival_dt as dt\n",
    "    ,cast(extract(year from arrival_dt) as int) as yr\n",
    "    ,cast(extract(month from arrival_dt) as int) as mth\n",
    "    ,cast(extract(day from arrival_dt) as int) as dy\n",
    "    --get the dates for each date field in the fact\n",
    "    from (select distinct arrival_dt from fact_immigration \n",
    "            union select distinct departure_dt from fact_immigration\n",
    "            union select distinct admitted_dt from fact_immigration) a\n",
    "    ;\n",
    "    delete from dim_date where dt is null or yr < 2000 --some cleaning\n",
    "    ;\n",
    "    ALTER TABLE dim_date ADD PRIMARY KEY (dt);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dims for the SAS look-up tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_country\n",
    "\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_country;\n",
    "    create table dim_country as \n",
    "    select a.country_cd as country_cd\n",
    "    ,trim(both from a.country) as country\n",
    "\n",
    "    from (select cast(country_cd as int) as country_cd\n",
    "            ,country \n",
    "            from country_sas)a\n",
    "    ;\n",
    "    ALTER TABLE dim_country ADD PRIMARY KEY (country_cd);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Create dim_airport dimension. We use the SAS data lookup in conjunction with \n",
    "the separate airport CSV data in order to get the most complete dataset for the final schema. \n",
    "The two datasets don't mesh perfectly and more cleaning could be done.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_airport\n",
    "    ;\n",
    "    create table dim_airport\n",
    "    as\n",
    "    select distinct a.airport_cd\n",
    "    ,coalesce(b.name,a.airport) as name\n",
    "    ,coalesce(b.municipality,left(a.airport,position(',' in a.airport)-1)) as city\n",
    "    ,coalesce(right(b.iso_region,2),\n",
    "        case when position(',' in a.airport) = length(a.airport)-3 --ensure this looks like a US airport with two letter state code\n",
    "                then trim(both from right(a.airport,length(a.airport) - position(',' in a.airport)))\n",
    "                else '-'\n",
    "                end) as state_cd\n",
    "    ,coalesce(b.type,'-') as type\n",
    "    from airport_sas a\n",
    "    left join airports_iata b on a.airport_cd = b.iata_code \n",
    "                            and b.type <> 'closed'\n",
    "                            and length(b.iso_region)=5 and substr(b.iso_region,1,3) = 'US-' --ensuring we're looking at US airports\n",
    "                            and right(b.iso_region,2) = right(a.airport,2) --check the state is the same. \n",
    "                            --NB. all of these checks because the airport code data in the SAS sources is not always a valid match with\n",
    "                            --the international standard iata_code\n",
    "    ;\n",
    "    ALTER TABLE dim_airport ADD PRIMARY KEY (airport_cd)\n",
    "    ;\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_arrival_mode\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_arrival_mode;\n",
    "    create table dim_arrival_mode\n",
    "    as\n",
    "    select distinct cast(trim(both from arrival_mode_cd) as smallint) as arrival_mode_cd\n",
    "    ,arrival_mode as arrival_mode_loc\n",
    "    from arrival_mode_sas\n",
    "    ;\n",
    "    ALTER TABLE dim_arrival_mode ADD PRIMARY KEY (arrival_mode_cd);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_visa_group\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_visa_group;\n",
    "    create table dim_visa_group\n",
    "    as\n",
    "    select distinct cast(visa_group_cd as smallint) as visa_group_cd\n",
    "    ,visa_group\n",
    "    from visa_group_sas\n",
    "    ;\n",
    "    ALTER TABLE dim_visa_group ADD PRIMARY KEY (visa_group_cd);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_us_state\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists dim_us_state;\n",
    "    create table dim_us_state\n",
    "    as\n",
    "    select distinct cast(us_state_cd as varchar(2)) as us_state_cd\n",
    "    ,us_state\n",
    "    from us_state_sas\n",
    "    ;\n",
    "    ALTER TABLE dim_us_state ADD PRIMARY KEY (us_state_cd);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_temperatures. NB. stripped out coordinates as too many cases of them being wrong, e.g. where \"Latitude\" = '50.63N'. \n",
    "# for future enhancements, wikipedia and other sites/APIs can provide accurate data.\n",
    "\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists fact_temperatures\n",
    "    ;    \n",
    "    create table fact_temperatures as \n",
    "    select distinct a.\"City\" as city\n",
    "    ,a.\"Country\" as country\n",
    "    ,cast(substr(a.\"dt\",1,4) as int)  as year\n",
    "    ,AVG(cast(a.\"AverageTemperature\" as decimal(12,2))) as average_temperature\n",
    "    from temperatures a\n",
    "    where 1=1\n",
    "    and substr(a.\"dt\",1,4) > '1999' --only data from this century\n",
    "    group by a.\"City\",a.\"Country\",substr(a.\"dt\",1,4)\n",
    "    ;\n",
    "    alter table fact_temperatures add primary key(city,country,year)\n",
    "    ;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_demographics. type conversions, removal of state name and flattening of the structure - in the source data each \"Race\"  \n",
    "# has one row per city. makes more sense to have a separate column for each. with larger volumes. Could have used one-hot encoding \n",
    "# to achieve this aim too.\n",
    "\n",
    "exec_psql(\"\"\"\n",
    "    drop table if exists fact_demographics\n",
    "    ;\n",
    "    create table fact_demographics as \n",
    "    select distinct a.\"City\" as city\n",
    "    ,cast(a.\"State Code\" as varchar(2)) as state_cd\n",
    "    ,cast(a.\"Median Age\" as decimal(4,2)) as median_age\n",
    "    ,cast(a.\"Male Population\" as int) as male_population\n",
    "    ,cast(a.\"Female Population\" as int) as female_population\n",
    "    ,cast(a.\"Total Population\" as int) as total_population\n",
    "    ,cast(b.\"Count\" as int) as hispanic_or_latino\n",
    "    ,cast(c.\"Count\" as int) as white\n",
    "    ,cast(d.\"Count\" as int) as black_or_african_american\n",
    "    ,cast(e.\"Count\" as int) as american_indian_and_alaska_native\n",
    "    ,cast(f.\"Count\" as int) as asian\n",
    "    ,cast(a.\"Number of Veterans\" as int) as number_of_veterans\n",
    "    ,cast(a.\"Foreign-born\" as int) as foreign_born\n",
    "    ,cast(a.\"Average Household Size\" as decimal(12,2)) as average_household_size\n",
    "    \n",
    "    from (select distinct a.\"City\"\n",
    "            ,a.\"Median Age\"\n",
    "            ,a.\"Male Population\"\n",
    "            ,a.\"Female Population\"\n",
    "            ,a.\"Total Population\"\n",
    "            ,a.\"Number of Veterans\"\n",
    "            ,a.\"Foreign-born\"\n",
    "            ,a.\"Average Household Size\"\n",
    "            ,a.\"State Code\"\n",
    "            from demographics a) a\n",
    "            \n",
    "    left join (select distinct a.\"City\"\n",
    "                ,a.\"State Code\"\n",
    "                ,a.\"Race\"\n",
    "                ,a.\"Count\"\n",
    "                from demographics a\n",
    "                where a.\"Race\" = 'Hispanic or Latino') b on a.\"City\" = b.\"City\" and a.\"State Code\" = b.\"State Code\" \n",
    "\n",
    "    left join (select distinct a.\"City\"\n",
    "                ,a.\"State Code\"\n",
    "                ,a.\"Race\"\n",
    "                ,a.\"Count\"\n",
    "                from demographics a\n",
    "                where a.\"Race\" = 'White') c on a.\"City\" = c.\"City\" and a.\"State Code\" = c.\"State Code\" \n",
    "\n",
    "    left join (select distinct a.\"City\"\n",
    "                ,a.\"State Code\"\n",
    "                ,a.\"Race\"\n",
    "                ,a.\"Count\"\n",
    "                from demographics a\n",
    "                where a.\"Race\" = 'Black or African-American') d on a.\"City\" = d.\"City\" and a.\"State Code\" = d.\"State Code\" \n",
    "    \n",
    "    left join (select distinct a.\"City\"\n",
    "                ,a.\"State Code\"\n",
    "                ,a.\"Race\"\n",
    "                ,a.\"Count\"\n",
    "                from demographics a\n",
    "                where a.\"Race\" = 'American Indian and Alaska Native') e on a.\"City\" = e.\"City\" and a.\"State Code\" = e.\"State Code\" \n",
    "                \n",
    "    left join (select distinct a.\"City\"\n",
    "                ,a.\"State Code\"\n",
    "                ,a.\"Race\"\n",
    "                ,a.\"Count\"\n",
    "                from demographics a\n",
    "                where a.\"Race\" = 'Asian') f on a.\"City\" = f.\"City\" and a.\"State Code\" = f.\"State Code\" \n",
    "    ;\n",
    "    alter table fact_demographics add primary key(city,state_cd)\n",
    "    ;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec_psql('drop table \"temperatures\";')\n",
    "# exec_psql('drop table \"demographics\";')\n",
    "# exec_psql('drop table \"airport_codes\";')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
