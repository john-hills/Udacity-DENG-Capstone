{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Engineering Capstone Project: Version 2.0\n",
    "\n",
    "### Tools and Libraries: AWS, Amazon S3 Buckets, Amazon Redshift, PySpark, Pandas, Boto3\n",
    "\n",
    "Following receipt of certification for my earlier work [here](https://github.com/john-hills/Udacity-DENG-Capstone/blob/master/Capstone%20-%20PostgreSQL%20-%20Local.ipynb), there were a few things that I still wanted to figure out. \n",
    "\n",
    "I was particularly interested in using AWS and the idea of 'Infrastructure as Code' that was introduced in the course. \n",
    "\n",
    "**Project Outline:**\n",
    "\n",
    "1. Create an IAM role to enable programmatic access of S3 and Redshift.\n",
    "2. Create an S3 bucket.\n",
    "3. Use PySpark to take source files and write them to parquet files in the S3 bucket.\n",
    "4. Create a Redshift Cluster.\n",
    "5. Copy the S3 parquet files into Redshift tables.\n",
    "\n",
    "The purpose of version 2.0 was to familiarise myself with S3, parquet files and Redshift. Having done that, I stopped and moved on to a new course. To see where I have designed and built a schema from the same source files, please see version 1.0 [here](https://github.com/john-hills/Udacity-DENG-Capstone/blob/master/Capstone%20-%20PostgreSQL%20-%20Local.ipynb).\n",
    "\n",
    "**Some set-up requirements:**\n",
    "\n",
    "1. Get an AWS account [here](https://aws.amazon.com/).\n",
    "2. Create an IAM user with Administrator privileges.\n",
    "3. Create a configuration file, 'dl.cfg', adding your own details as per the first code cell below.\n",
    "4. You need the source data files from the Udacity workspace. You can't access this without having enrolled on the course while some of the files are too big for me to provide them. There are some notes in the [README](https://github.com/john-hills/Udacity-DENG-Capstone) for this repository.\n",
    "\n",
    "That's it. This is the simplicity of using code with AWS. You can add, configure, use and remove any resources without logging on to the web console.\n",
    "\n",
    "*NB. Notes on configuration of Spark on a local machine can be found in the set-up process for my earlier work ([here](https://github.com/john-hills/Udacity-DENG-Capstone/blob/master/Capstone%20-%20PostgreSQL%20-%20Local.ipynb)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Load Parameters from the Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "## Run these installs if you haven't already installed the packages on your machine.\n",
    "\n",
    "#!pip install pyspark\n",
    "#conda install pyspark\n",
    "#!pip install boto3\n",
    "\n",
    "import configparser # to work with the configuration file\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, TimestampType as Timestamp, FloatType as Float\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import pyspark\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import logging\n",
    "%load_ext sql\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg') # edit this file to include your own values.\n",
    "\n",
    "# the environment variables below are set from those you entered into 'dl.cfg'\n",
    "# the first two are what boto3 uses to access your AWS account\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['AWS_DEFAULT_REGION']=config['AWS']['AWS_DEFAULT_REGION']\n",
    "\n",
    "REGION = config['AWS']['AWS_DEFAULT_REGION']\n",
    "\n",
    "# this is to ensure we can write to s3\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] \\\n",
    "    = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'\n",
    "\n",
    "\n",
    "# location of SAS immigration data\n",
    "SAS_DATA_LOCATION=config['OTHER']['SAS_DATA_LOCATION']\n",
    "\n",
    "# bucket name\n",
    "BUCKET_NAME = config.get(\"OTHER\",\"BUCKET_NAME\")\n",
    "\n",
    "# Redshift Cluster Configurations\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move the immigration data from sas7bdat format to CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = SAS_DATA_LOCATION + 'i94_apr16_sub.sas7bdat'\n",
    "\n",
    "df = pd.read_sas(fpath)\n",
    "str_df = df.select_dtypes([np.object])\n",
    "\n",
    "#convert bytes to strings\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "\n",
    "#replace byte cols with decoded versions in original df\n",
    "for col in str_df:\n",
    "    df[col] = str_df[col]\n",
    "\n",
    "df.to_csv('immigration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Set-up\n",
    "\n",
    "We're going to do everything using code rather than the management console.\n",
    "\n",
    "1. Create an IAM user with administrator access.\n",
    "2. Edit 'dl.cfg' to include the values appropriate for your account and machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create clients for IAM, EC2, S3 and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that AWS access key id, secret access key and default region can all be set within each statement.\n",
    "# I opted to set them in environment variables instead.\n",
    "\n",
    "ec2 = boto3.resource('ec2')\n",
    "s3 = boto3.resource('s3')\n",
    "iam = boto3.client('iam')\n",
    "redshift = boto3.client('redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IAM Role\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::524042478368:role/dwhRole'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that this will only work if the key/secret details in the configuration file\n",
    "# are those of an IAM user with Administrator access.\n",
    "\n",
    "def iam_redshift_role(iam_role_name):\n",
    "    \"\"\"\n",
    "    Create an IAM role for use with Redshift and attach S3 read only access.\n",
    "    \n",
    "    Params:\n",
    "    iam_role_name: the name label for the role\n",
    "    \n",
    "    Returns:\n",
    "    roleArn: The Amazon Resource Name (ARN) for the newly created IAM role.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        roles = iam.list_roles()\n",
    "        \n",
    "        # use a list comprehension to check if the role exists before trying to create it\n",
    "        if len([i['RoleName'] for i in roles['Roles'] if i['RoleName'] == iam_role_name]) == 0:\n",
    "            dwhRole = iam.create_role(\n",
    "                Path='/',\n",
    "                RoleName=iam_role_name,\n",
    "                Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "                AssumeRolePolicyDocument=json.dumps(\n",
    "                    {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                       'Effect': 'Allow',\n",
    "                       'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                     'Version': '2012-10-17'})\n",
    "            )    \n",
    "            \n",
    "            # attach the policy\n",
    "            iam.attach_role_policy(RoleName=iam_role_name,\n",
    "                               PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                              )['ResponseMetadata']['HTTPStatusCode']\n",
    "            \n",
    "        \n",
    "        elif len([i['RoleName'] for i in roles['Roles'] if i['RoleName'] == iam_role_name]) == 1:\n",
    "            print(f\"Role Name '{iam_role_name}' already exists.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # get the ARN for the new role\n",
    "    roleArn = iam.get_role(RoleName=iam_role_name)['Role']['Arn']\n",
    "    return roleArn\n",
    "\n",
    "\n",
    "roleArn = iam_redshift_role(DWH_IAM_ROLE_NAME)\n",
    "roleArn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an S3 bucket to which we can write parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket. Optionally specfify a region.\n",
    "\n",
    "    Parameters:\n",
    "    bucket_name: Bucket to create\n",
    "    region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    \n",
    "    Returns: True if it worked, False if there was an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def empty_and_delete_bucket(bucket_name):\n",
    "    \"\"\"\n",
    "    Remove all objects from a bucket and delete it.\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_name: Bucket to empty and delete\n",
    "    region: region bucket is in, e.g., 'us-west-2'\n",
    "    \n",
    "    Returns: True if it worked, False if there was an error.\n",
    "    \"\"\"\n",
    "        # Create bucket\n",
    "    try:\n",
    "        s3 = boto3.resource('s3')\n",
    "        bucket = s3.Bucket(bucket_name)\n",
    "        bucket.objects.delete()\n",
    "        bucket.delete()\n",
    "        print(f'{BUCKET_NAME}: deleted')\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def list_all_buckets():\n",
    "    s3 = boto3.resource('s3')\n",
    "    buckets = [bucket.name for bucket in s3.buckets.all()]\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bucket(BUCKET_NAME,'us-east-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my-working-bucket']\n"
     ]
    }
   ],
   "source": [
    "list_all_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move data from source to parquet files on Amazon S3 \n",
    "\n",
    "In the next cell, we take a number of steps to configure a spark session appropriately:\n",
    "\n",
    "    1. Create a SparkContext.\n",
    "    2. SparkContext contains the configurations that we need to update in order to be able to work with Amazon S3 buckets.\n",
    "    3. Once all the hadoop configurations ('hadoop_conf' lines below) have been changed, define a function that creates a Spark Session. \n",
    "    4. A Spark Session uses any existing SparkContext automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://gist.github.com/asmaier/5768c7cda3620901440a62248614bbd0\n",
    "sc=pyspark.SparkContext()\n",
    "\n",
    "# see https://github.com/databricks/spark-redshift/issues/298#issuecomment-271834485\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "# see https://stackoverflow.com/questions/28844631/how-to-set-hadoop-configuration-values-from-pyspark\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "\n",
    "# see https://stackoverflow.com/questions/43454117/how-do-you-use-s3a-with-spark-2-1-0-on-aws-us-east-2\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "# see http://blog.encomiabile.it/2015/10/29/apache-spark-amazon-s3-and-apache-mesos/\n",
    "hadoop_conf.set(\"fs.s3a.connection.maximum\", \"100000\")\n",
    "\n",
    "# see https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.\" + os.environ['AWS_DEFAULT_REGION'] + \".amazonaws.com\")\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a spark session in which to work on the data.\"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"C:/Progra~1/Java/jdk1.8.0_251/postgresql-42.2.14.jar\")\\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_s3(read_format,fpath,tname,delimiter=',',schema_name='nothing'):\n",
    "    \"\"\"\n",
    "    Create a Spark dataframe from an input file. \n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    read_format: E.g. csv.\n",
    "    fpath: Full path for your input file, e.g. 'c:\\your_file.csv'.\n",
    "    tname: The name of the file to write \n",
    "    delimiter: E.g. ','\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # only proceed with the write if the file has not yet been uploaded\n",
    "    if len(pd.DataFrame(get_matching_s3_objects(BUCKET_NAME,f'parquet/{tname}','parquet')).index) == 0:\n",
    "        \n",
    "        # we don't pass a schema if one is not provided\n",
    "        if schema_name == 'nothing':\n",
    "            df =spark.read.format(read_format) \\\n",
    "                      .option(\"header\",\"true\") \\\n",
    "                      .option(\"delimiter\",delimiter) \\\n",
    "                      .load(fpath)\n",
    "        else:\n",
    "            df =spark.read.format(read_format) \\\n",
    "                      .option(\"header\",\"true\") \\\n",
    "                      .option(\"delimiter\",delimiter) \\\n",
    "                      .schema(schema_name) \\\n",
    "                      .load(fpath)\n",
    "\n",
    "        df.write.mode(\"overwrite\").parquet('s3a://' + BUCKET_NAME + f'/parquet/{tname}.parquet')\n",
    "        print(tname,': written to parquet.')\n",
    "    else:\n",
    "        print(tname,': the parquet files already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Source: https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                break\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigration : written to parquet.\n",
      "temperatures : written to parquet.\n",
      "demographics : written to parquet.\n",
      "airports_iata : written to parquet.\n"
     ]
    }
   ],
   "source": [
    "# build schemas\n",
    "\n",
    "immigration_schema = R([\n",
    "    Fld(\"cicid\",Int()),\n",
    "    Fld(\"i94yr\",Str()),\n",
    "    Fld(\"i94mon\",Str()),\n",
    "    Fld(\"country_birth_cd\",Int()),\n",
    "    Fld(\"country_residence_cd\",Int()),\n",
    "    Fld(\"airport_cd\",Str()),\n",
    "    Fld(\"arrival_dt\",Date()),\n",
    "    Fld(\"arrival_mode_cd\",Int()),\n",
    "    Fld(\"state_cd\",Str()),\n",
    "    Fld(\"departure_dt\",Date()),\n",
    "    Fld(\"age_on_arrival\",Int()),\n",
    "    Fld(\"visa_group_cd\",Int()),\n",
    "    Fld(\"count\",Str()),\n",
    "    Fld(\"dtadfile\",Str()),\n",
    "    Fld(\"visapost\",Str()),\n",
    "    Fld(\"occupation\",Str()),\n",
    "    Fld(\"arrival_flag\",Str()),\n",
    "    Fld(\"entdepd\",Str()),\n",
    "    Fld(\"entdepu\",Str()),\n",
    "    Fld(\"matflag\",Str()),\n",
    "    Fld(\"birth_year\",Int()),\n",
    "    Fld(\"dtaddto\",Str()),\n",
    "    Fld(\"gender\",Str()),\n",
    "    Fld(\"insnum\",Str()),\n",
    "    Fld(\"airline\",Str()),\n",
    "    Fld(\"adm_num\",Str()),\n",
    "    Fld(\"flight_no\",Str()),\n",
    "    Fld(\"visatype\",Str())\n",
    "])\n",
    "\n",
    "demographics_schema = R([\n",
    "    Fld(\"city\",Str()),\n",
    "    Fld(\"state\",Str()),\n",
    "    Fld(\"median_age\",Str()),\n",
    "    Fld(\"male_population\",Str()),\n",
    "    Fld(\"female_population\",Str()),\n",
    "    Fld(\"total_population\",Str()),\n",
    "    Fld(\"number_of_veterans\",Str()),\n",
    "    Fld(\"foreign_born\",Str()),\n",
    "    Fld(\"average_household_size\",Str()),\n",
    "    Fld(\"state_code\",Str()),\n",
    "    Fld(\"race\",Str()),\n",
    "    Fld(\"count\",Str())    \n",
    "])\n",
    "\n",
    "temperature_schema = R([\n",
    "    Fld(\"date\",Str()),\n",
    "    Fld(\"average_temperature\",Float()),\n",
    "    Fld(\"average_temperature_uncertainty\",Str()),\n",
    "    Fld(\"city\",Str()),\n",
    "    Fld(\"country\",Str()),\n",
    "    Fld(\"latitude\",Str()),\n",
    "    Fld(\"longitude\",Str())\n",
    "])\n",
    "\n",
    "\n",
    "# build a dictionary of arguments for the four input files\n",
    "parameters_dict = {'immigration': {'read_format':'csv','fpath':'immigration.csv','delimiter':',','schema_name':immigration_schema},\n",
    "       'temperatures': {'read_format':'csv','fpath':'GlobalLandTemperaturesByCity.csv','delimiter':',','schema_name':temperature_schema},\n",
    "       'demographics': {'read_format':'csv','fpath':'us-cities-demographics.csv','delimiter':';','schema_name':demographics_schema},\n",
    "       'airports_iata': {'read_format':'csv','fpath':'airport-codes_csv.csv','delimiter':',','schema_name':'nothing'}\n",
    "      }\n",
    "\n",
    "# iterate through the dictionary, writing each dataframe to parquet files on S3. we can look at the schema for each staging table too.\n",
    "for k in parameters_dict.keys():\n",
    "       spark_to_s3(parameters_dict[k]['read_format'],parameters_dict[k]['fpath'],k,parameters_dict[k]['delimiter'],parameters_dict[k]['schema_name'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframes from the SAS look-up data in the file 'I94_SAS_Labels_Descriptions.SAS' (copied locally from Udacity workspace). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the lookup data\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "\n",
    "def code_mapper(file, idx):\n",
    "    \"\"\"\n",
    "    From udacity knowledge base.\n",
    "    Sources lookup values from the SAS file. \n",
    "    Returns a dictionary.\n",
    "    \"\"\"\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "\n",
    "i94cit = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business',\n",
    "'2': 'Pleasure',\n",
    "'3' : 'Student'}\n",
    "#correct value that's not right\n",
    "i94cit['582'] = 'MEXICO'\n",
    "\n",
    "#this creates a list that contains the dictionaries for each field and the new description for it.\n",
    "pairs = [(i94cit, 'country'), (i94port,'airport') #i94cit/i94res : use same lookup values\n",
    "         ,(i94mode,'arrival_mode'),(i94addr,'us_state'),(i94visa,'visa_group')]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "#get the lookup data into a pd dataframe and copy to PostgreSQL\n",
    "for (data, col) in pairs:\n",
    "    df = pd.DataFrame.from_dict(data, orient='index', columns=[col])\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    cols = df.columns\n",
    "    cols.values[0] = cols[1] + '_cd'\n",
    "    df.columns = cols\n",
    "    dfs[col] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift Clusters cost so remember to terminate this if you want to minimise costs.\n",
    "\n",
    "def create_redshift_cluster(cluster_type,node_type,num_nodes,cluster_identifier,db_user,db_password,roleArn=roleArn):\n",
    "    \"\"\"\n",
    "    Create a redshift cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster_type: type of cluster to create\n",
    "    node_type: type of node\n",
    "    num_nodes: number of nodes\n",
    "    cluster_identifier: the name for your cluster\n",
    "    db_user: database username\n",
    "    db_password: database password\n",
    "    roleArn: the IAM role that has the S3 access permission\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = redshift.create_cluster(        \n",
    "            #HW\n",
    "            ClusterType=DWH_CLUSTER_TYPE,\n",
    "            NodeType=DWH_NODE_TYPE,\n",
    "            NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "            #Identifiers & Credentials\n",
    "            DBName=DWH_DB,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DWH_DB_USER,\n",
    "            MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "            #Roles (for s3 access)\n",
    "            IamRoles=[roleArn]  \n",
    "            )\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to capture the endpoint of the new Redshift cluster\n",
    "\n",
    "The function checks periodically for 10 minutes and will not proceed with capturing the endpoint unless the cluster is 'available'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_redshift_endpoint(cluster_identifier,port=DWH_PORT):\n",
    "    \"\"\"\n",
    "    Check every 20 seconds for 10 minutes to see if the Redshift Cluster is available. When it is:\n",
    "    1. Use a list comprehension to capture some of the key properties of the Redshift Cluster.\n",
    "    2. Open an incoming TCP port to access the cluster endpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster_identifier: the name of the cluster you're interested in\n",
    "    port: port to access the database (default from configuration file)\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    #####################\n",
    "    ### the 'while' loop ensures that we don't progress unless the cluster status = 'available'\n",
    "    ### the function exits if that doesn't happen in time\n",
    "    #####################\n",
    "    while redshift.describe_clusters(ClusterIdentifier=cluster_identifier)['Clusters'][0]['ClusterStatus'] != 'available':\n",
    "        if i < 31:\n",
    "            i = i + 1\n",
    "            print(i,': ',redshift.describe_clusters(ClusterIdentifier=cluster_identifier)['Clusters'][0]['ClusterStatus'],': Waiting 20 seconds.')\n",
    "            time.sleep(20) # wait 20 seconds\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=cluster_identifier)['Clusters'][0]\n",
    "    # these are the parameters that we want to see\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    \n",
    "    # when the function is called, all possible parameters are passed in 'myClusterProps'\n",
    "    # if the parameter is in 'keysToShow' then it is added to 'x'\n",
    "    x = [(k, v) for k,v in myClusterProps.items() if k in keysToShow]\n",
    "    \n",
    "    # once the list comprehension has iterated through each item in 'myClusterProps', 'x' is used to populate a \n",
    "    # dataframe which can be viewed\n",
    "    df = pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "    #make the endpoint available outside this function\n",
    "    global DWH_ENDPOINT\n",
    "    DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "    DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "    print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "    print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)\n",
    "    \n",
    "    # Open an incoming TCP port to access the cluster endpoint\n",
    "    # This only needs to be run once. An exception is expected if you've already run it.\n",
    "    try:\n",
    "        vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "        defaultSg = list(vpc.security_groups.all())[0]\n",
    "        print('ec2 SecurityGroup :: ',defaultSg)\n",
    "        defaultSg.authorize_ingress(\n",
    "            GroupName=defaultSg.group_name,\n",
    "            CidrIp='0.0.0.0/0',\n",
    "            IpProtocol='TCP',\n",
    "            FromPort=int(port),\n",
    "            ToPort=int(port)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Redshift Cluster, wait for it to be ready and then capture the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'capture_new_aws_obj' is not defined\n",
      "1 :  creating : Waiting 20 seconds.\n",
      "2 :  creating : Waiting 20 seconds.\n",
      "3 :  creating : Waiting 20 seconds.\n",
      "4 :  creating : Waiting 20 seconds.\n",
      "5 :  creating : Waiting 20 seconds.\n",
      "6 :  creating : Waiting 20 seconds.\n",
      "7 :  creating : Waiting 20 seconds.\n",
      "8 :  creating : Waiting 20 seconds.\n",
      "9 :  creating : Waiting 20 seconds.\n",
      "10 :  creating : Waiting 20 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-55c01a3e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::524042478368:role/dwhRole\n",
      "ec2 SecurityGroup ::  ec2.SecurityGroup(id='sg-4dcd2737')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "create_redshift_cluster(DWH_CLUSTER_TYPE,DWH_NODE_TYPE,DWH_NUM_NODES,DWH_CLUSTER_IDENTIFIER,DWH_DB_USER,DWH_DB_PASSWORD)\n",
    "capture_redshift_endpoint(DWH_CLUSTER_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "conn_string=f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASSWORD}@{DWH_ENDPOINT}:{DWH_PORT}/{DWH_DB}\"\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the S3 files to Redshift.\n",
    "\n",
    "First, we create some schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS staging_s3;\n",
    "CREATE SCHEMA IF NOT EXISTS staging_pd;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the staging tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- S3 source\n",
    "--SET search_path TO staging_s3;\n",
    "DROP TABLE IF EXISTS staging_s3.immigration;\n",
    "DROP TABLE IF EXISTS staging_s3.demographics;\n",
    "DROP TABLE IF EXISTS staging_s3.temperature;\n",
    "DROP TABLE IF EXISTS staging_s3.airports_iata;\n",
    "\n",
    "CREATE TABLE staging_s3.immigration (\n",
    "    cicid integer distkey sortkey,\n",
    "    i94yr varchar(50),\n",
    "    i94mon varchar(50),\n",
    "    country_birth_cd integer,\n",
    "    country_residence_cd integer,\n",
    "    airport_cd varchar(50),\n",
    "    arrival_dt datetime,\n",
    "    arrival_mode_cd integer,\n",
    "    state_cd varchar(50),\n",
    "    departure_dt datetime,\n",
    "    age_on_arrival integer,\n",
    "    visa_group_cd integer,\n",
    "    count varchar(50),\n",
    "    dtadfile varchar(50),\n",
    "    visapost varchar(50),\n",
    "    occupation varchar(50),\n",
    "    arrival_flag varchar(50),\n",
    "    entdepd varchar(50),\n",
    "    entdepu varchar(50),\n",
    "    matflag varchar(50),\n",
    "    birth_year integer,\n",
    "    dtaddto varchar(50),\n",
    "    gender varchar(50),\n",
    "    insnum varchar(50),\n",
    "    airline varchar(50),\n",
    "    adm_num varchar(50),\n",
    "    flight_no varchar(50),\n",
    "    visatype varchar(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE staging_s3.demographics (\n",
    "\n",
    "    city varchar(50) not null sortkey distkey,\n",
    "    state varchar(50) not null,\n",
    "    median_age varchar(50),\n",
    "    male_population varchar(50),\n",
    "    female_population varchar(50),\n",
    "    total_population varchar(50),\n",
    "    number_of_veterans varchar(50),\n",
    "    foreign_born varchar(50),\n",
    "    average_household_size varchar(50),\n",
    "    state_code varchar(50),\n",
    "    race varchar(50),\n",
    "    count varchar(50)        \n",
    ")\n",
    ";\n",
    "\n",
    "CREATE TABLE staging_s3.temperature (\n",
    "\n",
    "    date varchar(50) sortkey distkey,\n",
    "    average_temperature float,\n",
    "    average_temperature_uncertainty varchar(50),\n",
    "    city nvarchar(200),\n",
    "    country varchar(200),\n",
    "    latitude varchar(50),\n",
    "    longitude varchar(50)    \n",
    ")\n",
    "--diststyle all\n",
    ";\n",
    "\n",
    "CREATE TABLE staging_s3.airports_iata (\n",
    "    ident varchar(50) not null sortkey distkey,\n",
    "    type varchar(50),\n",
    "    name varchar(128),\n",
    "    elevation_ft varchar(50),\n",
    "    continent varchar(50),\n",
    "    iso_country varchar(50),\n",
    "    iso_region varchar(50),\n",
    "    municipality varchar(60),\n",
    "    gps_code varchar(50),\n",
    "    iata_code varchar(50),\n",
    "    local_code varchar(50),\n",
    "    coordinates varchar(50)\n",
    "\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to copy the files to Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def loadTables_S3(schema, tables):\n",
    "    \"\"\"\n",
    "    Function to load S3 data into Redshift\n",
    "    \n",
    "    Parameters:\n",
    "    schema: the database schema where the tables will be created\n",
    "    tables: the list of tables to iterate through for creation\n",
    "    \"\"\"\n",
    "    loadTimes = []\n",
    "    SQL_SET_SCEMA = \"SET search_path TO {};\".format(schema)\n",
    "    %sql $SQL_SET_SCEMA\n",
    "    \n",
    "    for table in tables:\n",
    "        SQL_COPY = f\"\"\"\n",
    "        copy {schema}.{table} from 's3://my-working-bucket/parquet/{table}' \n",
    "        IAM_ROLE '{roleArn}'\n",
    "        FORMAT PARQUET;\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======\".format(table, schema))\n",
    "        print(SQL_COPY)\n",
    "\n",
    "        t0 = time()\n",
    "        %sql $SQL_COPY\n",
    "        loadTime = time()-t0\n",
    "        loadTimes.append(loadTime)\n",
    "\n",
    "        print(\"=== DONE IN: {0:.2f} sec\\n\".format(loadTime))\n",
    "    return pd.DataFrame({\"table\":tables, \"loadtime_\"+schema:loadTimes}).set_index('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final step. Create a list of the tables we want to create and then call the function to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "======= LOADING TABLE: ** immigration ** IN SCHEMA ==> staging_s3 =======\n",
      "\n",
      "        copy staging_s3.immigration from 's3://my-working-bucket/parquet/immigration' \n",
      "        IAM_ROLE 'arn:aws:iam::524042478368:role/dwhRole'\n",
      "        FORMAT PARQUET;\n",
      "        \n",
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 6.32 sec\n",
      "\n",
      "======= LOADING TABLE: ** demographics ** IN SCHEMA ==> staging_s3 =======\n",
      "\n",
      "        copy staging_s3.demographics from 's3://my-working-bucket/parquet/demographics' \n",
      "        IAM_ROLE 'arn:aws:iam::524042478368:role/dwhRole'\n",
      "        FORMAT PARQUET;\n",
      "        \n",
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 1.14 sec\n",
      "\n",
      "======= LOADING TABLE: ** temperature ** IN SCHEMA ==> staging_s3 =======\n",
      "\n",
      "        copy staging_s3.temperature from 's3://my-working-bucket/parquet/temperature' \n",
      "        IAM_ROLE 'arn:aws:iam::524042478368:role/dwhRole'\n",
      "        FORMAT PARQUET;\n",
      "        \n",
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 4.90 sec\n",
      "\n",
      "======= LOADING TABLE: ** airports_iata ** IN SCHEMA ==> staging_s3 =======\n",
      "\n",
      "        copy staging_s3.airports_iata from 's3://my-working-bucket/parquet/airports_iata' \n",
      "        IAM_ROLE 'arn:aws:iam::524042478368:role/dwhRole'\n",
      "        FORMAT PARQUET;\n",
      "        \n",
      " * postgresql://dwhuser:***@dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 12.52 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loadtime_staging_s3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>immigration</td>\n",
       "      <td>6.316521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>demographics</td>\n",
       "      <td>1.136695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>temperature</td>\n",
       "      <td>4.896018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>airports_iata</td>\n",
       "      <td>12.517650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loadtime_staging_s3\n",
       "table                             \n",
       "immigration    6.316521           \n",
       "demographics   1.136695           \n",
       "temperature    4.896018           \n",
       "airports_iata  12.517650          "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_tables = ['immigration','demographics','temperature','airports_iata']\n",
    "loadTables_S3('staging_s3',s3_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the S3 bucket path must point to a folder containing the .parquet file, \n",
    "not the file itself. Any file found in the folder is assumed to be uploaded as a parquet file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up: only run this when you're done.\n",
    "\n",
    "Delete the IAM role, the S3 bucket and the Redshift Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-working-bucket: deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'dwhuser',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwhcluster.ca6rvm6jz4xb.us-east-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2020, 7, 22, 8, 35, 2, 313000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-4dcd2737',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-55c01a3e',\n",
       "  'AvailabilityZone': 'us-east-2b',\n",
       "  'PreferredMaintenanceWindow': 'sat:06:00-sat:06:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::524042478368:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2020, 7, 25, 6, 0, tzinfo=tzutc())},\n",
       " 'ResponseMetadata': {'RequestId': '1e4fbe1d-c995-461f-a4dc-a3e69c366571',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1e4fbe1d-c995-461f-a4dc-a3e69c366571',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2382',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Wed, 22 Jul 2020 17:39:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete IAM role\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "\n",
    "#delete s3 bucket\n",
    "empty_and_delete_bucket(BUCKET_NAME)\n",
    "\n",
    "#delete Redshift cluster\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
